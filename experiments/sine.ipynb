{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a8c5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e765ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "from typing import Iterator\n",
    "\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from datasets.sine import create_data_factory, load_sine, SineLoaderConfig\n",
    "from rubicon.nns.mlp import LayerConfig, MLPConfig, MultiLayerPerceptron\n",
    "from rubicon.nns._base import TrainingConfig, NTKTrainingConfig\n",
    "from rubicon.nns.metrics.mae import MeanAbsoluteError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f5fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_shape(\n",
    "    iterator: Iterator[tuple[jnp.ndarray, jnp.ndarray]],\n",
    ") -> tuple[int, ...]:\n",
    "    \"\"\"Extract the input shape from the first batch of an iterator\"\"\"\n",
    "    try:\n",
    "        x_batch, _ = next(iter(iterator))\n",
    "        return x_batch.shape[:]\n",
    "    except StopIteration:\n",
    "        raise ValueError(\"Empty iterator; cannot determine input shape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9b885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the input shape, necessary for the model initialization.\n",
    "batch_size = 32\n",
    "n_train = 1600\n",
    "n_test = 320\n",
    "dataset_config = SineLoaderConfig(\n",
    "    batch_size=batch_size,\n",
    "    n_train=n_train,\n",
    "    n_test=n_test,\n",
    ")\n",
    "temp_train_iter, _ = load_sine(dataset_config)\n",
    "input_shape = get_input_shape(temp_train_iter)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5c2a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MLPConfig(\n",
    "    output_layer=LayerConfig(size=1), hidden_layers=[LayerConfig(size=256)]\n",
    ")\n",
    "model = MultiLayerPerceptron(config)\n",
    "model(input_shape=input_shape)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388eee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard training\n",
    "training_config = TrainingConfig(\n",
    "    data_factory=create_data_factory(dataset_config),\n",
    "    num_epochs=2,\n",
    "    batch_size=batch_size,\n",
    "    verbose=True,\n",
    "    accuracy_fn=MeanAbsoluteError(),\n",
    ")\n",
    "history = model.fit(training_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ed3969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training with kare\n",
    "kare_training_config = NTKTrainingConfig(\n",
    "    data_factory=create_data_factory(dataset_config),\n",
    "    num_epochs=2,\n",
    "    batch_size=batch_size,\n",
    "    verbose=True,\n",
    "    accuracy_fn=MeanAbsoluteError(),\n",
    "    z=1e-3,\n",
    "    lambd=1e-6,\n",
    "    update_params=False,\n",
    "    with_kare=True,\n",
    ")\n",
    "history = model.fit(kare_training_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e07e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import optax\n",
    "from rubicon.nns.losses import KARELoss, MSELoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd68f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(p, x):\n",
    "    def _compute_gradient(p, x):\n",
    "        grad_fn = jax.grad(lambda p, x: model.apply_fn(p, x).sum())\n",
    "        flat_grads = []\n",
    "        for item in grad_fn(p, x):\n",
    "            for element in item:\n",
    "                flat_grads.append(jnp.ravel(element))\n",
    "        return jnp.concatenate(flat_grads)\n",
    "\n",
    "    per_sample_grads = jax.vmap(lambda p, x: _compute_gradient(p, x))\n",
    "    return per_sample_grads(p, x)\n",
    "    \n",
    "\n",
    "def compute_ntk(params, x1, x2):\n",
    "    \"\"\"Compute the empirical NTK between batches of pairs of points.\"\"\"\n",
    "    G1 = compute_gradient(params, x1)\n",
    "    G2 = compute_gradient(params, x2)\n",
    "\n",
    "    def _compute_ntk(g1, g2):\n",
    "        \"\"\"Calculate as the scalar product of the two gradients.\"\"\"\n",
    "        return g1[:, None].T @ g2[:, None]\n",
    "    \n",
    "    per_sample_ntk = jax.vmap(lambda g1, g2: _compute_ntk(g1, g2))\n",
    "    return per_sample_ntk(G1, G2)\n",
    "\n",
    "def kare_loss(p, x, y, z):\n",
    "    def _kare_loss(y, K, z):\n",
    "        \"\"\"Compute KARE pointwise.\"\"\"\n",
    "        n = K.shape[0]\n",
    "        K_norm = K / n\n",
    "        mat = K_norm + z * jnp.eye(n)\n",
    "        inv = jax.jit(jnp.linalg.inv, backend=\"cpu\")(mat)\n",
    "        inv2 = inv @ inv\n",
    "        return ((1/n) * y.T @ inv2 @ y) / ((1/n) * jnp.trace(inv)) ** 2\n",
    "    \n",
    "    K = compute_ntk(p, x, x)\n",
    "    # Vectorization of the pointwise calculation\n",
    "    per_sample_kare = jax.vmap(lambda y, K, z: _kare_loss(y, K, z))\n",
    "    return jnp.sum(per_sample_kare(y, K, z))\n",
    "\n",
    "grad_kare = jax.grad(kare_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a91877",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.params:\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476099b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafactory = create_data_factory(dataset_config)\n",
    "train_iter, _ = datafactory()\n",
    "params = model.params\n",
    "z = 1e-3\n",
    "optimizer = optax.adam(learning_rate=1e-3)\n",
    "opt_state = optimizer.init(model.params)\n",
    "\n",
    "for x, y in train_iter:\n",
    "    grads = grad_kare(params, x, y, z)\n",
    "    updates, opt_state = optax.update(grads, opt_state) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aca0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_iter = create_data_factory(dataset_config)()\n",
    "z = 1e-3\n",
    "\n",
    "# computing the gradient, then flatten, in batches\n",
    "def flat_grad(p, x):\n",
    "    grad_fn = jax.grad(lambda p, x: model.apply_fn(p, x).sum())\n",
    "    flat_grads = []\n",
    "    for item in grad_fn(p, x):\n",
    "        for element in item:\n",
    "            flat_grads.append(jnp.ravel(element))\n",
    "    return jnp.concatenate(flat_grads)\n",
    "\n",
    "xs, ys = next(test_iter)\n",
    "per_sample_grads = jax.vmap(lambda p, x: flat_grad(model.params, x))\n",
    "grads = per_sample_grads(model.params, xs.squeeze())\n",
    "grads.shape  # (32, 769)\n",
    "\n",
    "# compute the ntk in batches\n",
    "def compute_ntk(x1, x2, params):\n",
    "    G1 = per_sample_grads(x1.squeeze(), params)  # (32, 769)\n",
    "    G2 = per_sample_grads(x2.squeeze(), params)  # (32, 769)\n",
    "\n",
    "    def _compute_ntk(g1, g2):\n",
    "        return g1[:, None].T @ g2[:, None]\n",
    "    \n",
    "    per_sample_ntk = jax.vmap(lambda g1, g2: _compute_ntk(g1, g2))\n",
    "    return per_sample_ntk(G1, G2)\n",
    "\n",
    "x1, _ = next(test_iter)\n",
    "x2, _ = next(test_iter)\n",
    "ntks = compute_ntk(x1, x2)\n",
    "ntks.shape  # (32, 1, 1)\n",
    "\n",
    "# compute an implementation of kare that supports batches\n",
    "def kare(y, K, z):\n",
    "    def _kare(y, K, z):\n",
    "        n = K.shape[0]\n",
    "        K_norm = K / n\n",
    "        mat = K_norm + z * jnp.eye(n)\n",
    "        inv = jax.jit(jnp.linalg.inv, backend=\"cpu\")(mat)\n",
    "        inv2 = inv @ inv\n",
    "        return ((1/n) * y.T @ inv2 @ y) / ((1/n) * jnp.trace(inv)) ** 2\n",
    "    \n",
    "    per_sample_kare = jax.vmap(lambda y, K: _kare(y, K, z))\n",
    "    return jnp.sum(per_sample_kare(y, K))\n",
    "\n",
    "@jax.jit\n",
    "def compute_kare(x, y, z):\n",
    "    K = compute_ntk(x, x)\n",
    "    return kare(y, K, z).squeeze()\n",
    "\n",
    "grad_kare = jax.grad(compute_kare)\n",
    "optimizer = optax.adam(learning_rate=1e-3)\n",
    "opt_state = optimizer.init(model.params)\n",
    "\n",
    "params = model.params\n",
    "\n",
    "x, y = next(test_iter)\n",
    "grads = grad_kare(x, y, z)\n",
    "print(grads)\n",
    "\n",
    "# for x, y in test_iter:\n",
    "#     grads = grad_kare(x, y, z)\n",
    "#     grads = list(grads.squeeze())\n",
    "#     print(grads)\n",
    "#     # updates, opt_state = optimizer.update(grads, opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8684be0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf8a0b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f30bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafactory = create_data_factory(dataset_config)\n",
    "train_iter, _ = datafactory()\n",
    "params = model.params\n",
    "z = 1e-3\n",
    "optimizer = optax.adam(learning_rate=1e-3)\n",
    "opt_state = optimizer.init(model.params)\n",
    "\n",
    "\n",
    "# for x, y in train_iter:\n",
    "#     grads = grad_kare(params, x, y, z)\n",
    "#     updates, opt_state = optax.update(grads, opt_state) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fa65d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafactory = create_data_factory(dataset_config)\n",
    "train_iter, _ = datafactory()\n",
    "\n",
    "def compute_gradient(f, p, xs) -> jnp.ndarray:\n",
    "    grad_fn = jax.grad(lambda p, x: f(p, x).squeeze())\n",
    "\n",
    "    def _pointwise(x):\n",
    "        flat_grads = []\n",
    "        for layer in grad_fn(p, x):\n",
    "            for part in layer:\n",
    "                flat_grads.append(part.flatten())\n",
    "        return jnp.concatenate(flat_grads)\n",
    "\n",
    "    per_sample = jax.vmap(_pointwise, in_axes=0)\n",
    "    return per_sample(xs)\n",
    "\n",
    "def compute_ntk(p, xs1, xs2) -> jnp.ndarray:\n",
    "    G1 = compute_gradient(model.apply_fn, p, xs1)\n",
    "    G2 = compute_gradient(model.apply_fn, p, xs2)\n",
    "    return G1.dot(G2.T)\n",
    "\n",
    "def kare(p, x, y, z):  # y.shape=(32,1), K.shape=(32,32), z.shape=()\n",
    "    K = compute_ntk(p, x, x)\n",
    "    n = K.shape[0]\n",
    "    K_norm = K / n\n",
    "    mat = K_norm + z * jnp.eye(n)\n",
    "    inv = jax.jit(jnp.linalg.inv, backend=\"cpu\")(mat)\n",
    "    inv2 = inv @ inv\n",
    "    return (((1/n) * y.T @ inv2 @ y) / ((1/n) * jnp.trace(inv)) ** 2)[0, 0]\n",
    "\n",
    "xs, ys = next(train_iter)\n",
    "kare(model.params, xs, ys, 1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4740f2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafactory = create_data_factory(dataset_config)\n",
    "train_iter, _ = datafactory()\n",
    "params = model.params\n",
    "z = 1e-3\n",
    "optimizer = optax.adam(learning_rate=1e-3)\n",
    "opt_state = optimizer.init(model.params)\n",
    "grad_kare = jax.grad(kare)\n",
    "\n",
    "xs, ys = next(train_iter)\n",
    "grads = grad_kare(model.params, x, y, z)\n",
    "print(len(grads))\n",
    "print(grads)\n",
    "\n",
    "# for x, y in train_iter:\n",
    "#     grads = grad_kare(model.params, x, y, z)\n",
    "#     updates, opt_state = optimizer.update(grads, opt_state)\n",
    "#     params = optax.apply_updates(params, updates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df094014",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
